{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\Anthony\\Documents\\CS Homework\\AI 260 Deep Learning S2024\\Transfer-Learning-6\\deepLearning\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\Anthony\\Documents\\CS Homework\\AI 260 Deep Learning S2024\\Transfer-Learning-6\\deepLearning\\Lib\\site-packages\\keras\\src\\backend.py:1398: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\Anthony\\Documents\\CS Homework\\AI 260 Deep Learning S2024\\Transfer-Learning-6\\deepLearning\\Lib\\site-packages\\keras\\src\\layers\\pooling\\max_pooling2d.py:161: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load the Pre-trained VGG16 Model\n",
    "# import VGG16 from keras.applications and load the model with ImageNet weights, excluding the top (fully connected) layers.\n",
    "\n",
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.image import resize\n",
    "from cv2 import imread, imshow\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Load the VGG16 model, pre-trained weights, exclude top layers\n",
    "base_model = VGG16(weights='imagenet', include_top=False, input_shape=(224, 224, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add Custom Layers\n",
    "# Freeze the layers of the base model to prevent them from being updated during training, then add your custom layers on top for the classification task.\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "# Freeze the base model\n",
    "base_model.trainable = False\n",
    "\n",
    "# Create a custom head for our network\n",
    "model = models.Sequential([\n",
    "  base_model,\n",
    "  layers.GlobalAveragePooling2D(),\n",
    "  layers.Dense(1024, activation='relu'),\n",
    "  layers.Dropout(0.2),\n",
    "  layers.Dense(2, activation='softmax')  # Assuming two classes: Cat and Dog\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\Anthony\\Documents\\CS Homework\\AI 260 Deep Learning S2024\\Transfer-Learning-6\\deepLearning\\Lib\\site-packages\\keras\\src\\optimizers\\__init__.py:309: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Compile and Train the Model\n",
    "# Compile the model, specifying the loss function, optimizer, and metrics. Then, train the model with your dataset.\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images, test_images, train_labels, test_labels = [], [], [], []\n",
    "DIR = 'dataset/'\n",
    "\n",
    "def split_train_test():\n",
    "    for root, dir, files in os.walk('./dataset'):\n",
    "        for file in files:\n",
    "            img = imread(DIR + file)\n",
    "            processed = resize(\n",
    "                np.expand_dims(img, axis=0), [224, 224]\n",
    "            )\n",
    "            cat_or_dog = 1 if file.startswith('cat') else 0\n",
    "            # There are 12500 cats and dogs (indexed 0 to 12499) in the data set.\n",
    "            # To make an 80%:20% train-test split, 10000 cats and 10000 dogs need to go into the train set.\n",
    "            # All other cat and dog images go into the test set.\n",
    "            if int(file[file.index('.')+1:file.rindex('.')]) < 10000:\n",
    "                train_images.append(processed)\n",
    "                train_labels.append(cat_or_dog)\n",
    "            else:\n",
    "                test_images.append(processed)\n",
    "                test_labels.append(cat_or_dog)\n",
    "\n",
    "split_train_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "tf.Tensor(\n",
      "[[[[ 40.125     44.125     39.125   ]\n",
      "   [ 41.58817   44.061382  39.570312]\n",
      "   [ 43.848213  44.848213  40.848213]\n",
      "   ...\n",
      "   [180.87604  206.7868   210.63501 ]\n",
      "   [169.12376  201.10594  204.10594 ]\n",
      "   [163.03015  197.69086  200.69086 ]]\n",
      "\n",
      "  [[ 41.        45.        40.      ]\n",
      "   [ 42.017857  44.49107   40.      ]\n",
      "   [ 43.530132  44.530132  40.530132]\n",
      "   ...\n",
      "   [174.55463  200.35153  204.25667 ]\n",
      "   [165.6651   196.71312  200.27228 ]\n",
      "   [160.67636  194.21207  197.96207 ]]\n",
      "\n",
      "  [[ 40.268974  44.268974  39.268974]\n",
      "   [ 40.767857  43.24107   38.75    ]\n",
      "   [ 41.219868  42.219868  38.219868]\n",
      "   ...\n",
      "   [168.77898  192.79578  197.3259  ]\n",
      "   [164.99554  194.48663  198.97772 ]\n",
      "   [165.66518  197.32588  202.32588 ]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[ 28.785715  27.785715  29.785715]\n",
      "   [ 25.725447  24.725447  26.725447]\n",
      "   [ 21.830357  20.830357  22.830357]\n",
      "   ...\n",
      "   [ 25.214233  31.214233  44.214233]\n",
      "   [ 21.107178  27.107178  39.125   ]\n",
      "   [ 32.815792  38.815792  49.815792]]\n",
      "\n",
      "  [[ 30.660715  29.660715  31.660715]\n",
      "   [ 26.964287  25.964287  27.964287]\n",
      "   [ 21.925224  20.925224  22.925224]\n",
      "   ...\n",
      "   [ 20.878334  28.128334  41.128334]\n",
      "   [ 22.01683   29.573761  41.284653]\n",
      "   [ 35.315792  43.190792  53.565792]]\n",
      "\n",
      "  [[ 30.512278  29.512278  31.512278]\n",
      "   [ 26.089287  25.089287  27.089287]\n",
      "   [ 20.732143  19.732143  21.732143]\n",
      "   ...\n",
      "   [ 35.02449   43.02449   56.02449 ]\n",
      "   [ 26.112625  34.603714  46.130447]\n",
      "   [ 22.938618  31.938618  41.938618]]]], shape=(1, 224, 224, 3), dtype=float32)\n",
      "1\n",
      "[[[ 40.125     44.125     39.125   ]\n",
      "  [ 41.58817   44.061382  39.570312]\n",
      "  [ 43.848213  44.848213  40.848213]\n",
      "  ...\n",
      "  [180.87604  206.7868   210.63501 ]\n",
      "  [169.12376  201.10594  204.10594 ]\n",
      "  [163.03015  197.69086  200.69086 ]]\n",
      "\n",
      " [[ 41.        45.        40.      ]\n",
      "  [ 42.017857  44.49107   40.      ]\n",
      "  [ 43.530132  44.530132  40.530132]\n",
      "  ...\n",
      "  [174.55463  200.35153  204.25667 ]\n",
      "  [165.6651   196.71312  200.27228 ]\n",
      "  [160.67636  194.21207  197.96207 ]]\n",
      "\n",
      " [[ 40.268974  44.268974  39.268974]\n",
      "  [ 40.767857  43.24107   38.75    ]\n",
      "  [ 41.219868  42.219868  38.219868]\n",
      "  ...\n",
      "  [168.77898  192.79578  197.3259  ]\n",
      "  [164.99554  194.48663  198.97772 ]\n",
      "  [165.66518  197.32588  202.32588 ]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[ 28.785715  27.785715  29.785715]\n",
      "  [ 25.725447  24.725447  26.725447]\n",
      "  [ 21.830357  20.830357  22.830357]\n",
      "  ...\n",
      "  [ 25.214233  31.214233  44.214233]\n",
      "  [ 21.107178  27.107178  39.125   ]\n",
      "  [ 32.815792  38.815792  49.815792]]\n",
      "\n",
      " [[ 30.660715  29.660715  31.660715]\n",
      "  [ 26.964287  25.964287  27.964287]\n",
      "  [ 21.925224  20.925224  22.925224]\n",
      "  ...\n",
      "  [ 20.878334  28.128334  41.128334]\n",
      "  [ 22.01683   29.573761  41.284653]\n",
      "  [ 35.315792  43.190792  53.565792]]\n",
      "\n",
      " [[ 30.512278  29.512278  31.512278]\n",
      "  [ 26.089287  25.089287  27.089287]\n",
      "  [ 20.732143  19.732143  21.732143]\n",
      "  ...\n",
      "  [ 35.02449   43.02449   56.02449 ]\n",
      "  [ 26.112625  34.603714  46.130447]\n",
      "  [ 22.938618  31.938618  41.938618]]]\n"
     ]
    }
   ],
   "source": [
    "print(train_labels[1])\n",
    "print(train_images[1])\n",
    "train_labels_np = np.asarray(train_labels)\n",
    "train_images_np = np.array(train_images,dtype=np.float32).reshape((20000,224,224,3))\n",
    "print(train_labels_np[1])\n",
    "print(train_images_np[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 11.2 GiB for an array with shape (20000, 224, 224, 3) and data type float32",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Assuming `train_images`, `train_labels` is your training dataset\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_images_np\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_labels_np\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtest_images\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtest_labels\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Anthony\\Documents\\CS Homework\\AI 260 Deep Learning S2024\\Transfer-Learning-6\\deepLearning\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Users\\Anthony\\Documents\\CS Homework\\AI 260 Deep Learning S2024\\Transfer-Learning-6\\deepLearning\\Lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py:91\u001b[0m, in \u001b[0;36mconvert_to_eager_tensor\u001b[1;34m(value, ctx, dtype)\u001b[0m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Converts the given `value` to an `EagerTensor`.\u001b[39;00m\n\u001b[0;32m     72\u001b[0m \n\u001b[0;32m     73\u001b[0m \u001b[38;5;124;03mNote that this function could return cached copies of created constants for\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     85\u001b[0m \u001b[38;5;124;03m  TypeError: if `dtype` is not compatible with the type of t.\u001b[39;00m\n\u001b[0;32m     86\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     87\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value, np\u001b[38;5;241m.\u001b[39mndarray):\n\u001b[0;32m     88\u001b[0m   \u001b[38;5;66;03m# Make a copy explicitly because the EagerTensor might share the underlying\u001b[39;00m\n\u001b[0;32m     89\u001b[0m   \u001b[38;5;66;03m# memory with the input array. Without this copy, users will be able to\u001b[39;00m\n\u001b[0;32m     90\u001b[0m   \u001b[38;5;66;03m# modify the EagerTensor after its creation by changing the input array.\u001b[39;00m\n\u001b[1;32m---> 91\u001b[0m   value \u001b[38;5;241m=\u001b[39m value\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[0;32m     92\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value, ops\u001b[38;5;241m.\u001b[39mEagerTensor):\n\u001b[0;32m     93\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m value\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m!=\u001b[39m dtype:\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 11.2 GiB for an array with shape (20000, 224, 224, 3) and data type float32"
     ]
    }
   ],
   "source": [
    "# Assuming `train_images`, `train_labels` is your training dataset\n",
    "model.fit(train_images_np, train_labels_np, epochs=1, validation_data=(test_images,test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the Model\n",
    "# After training, evaluate your model's performance on a separate test dataset to see how well it performs.\n",
    "# Assuming `test_images`, `test_labels` is your testing dataset\n",
    "test_loss, test_acc = model.evaluate(test_images, test_labels, verbose=2)\n",
    "print('\\nTest accuracy:', test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save and Load Your Model\n",
    "# Finally, save your trained model for later use and load it whenever you need to classify new images.\n",
    "model.save('cats_vs_dogs_model.h5')\n",
    "\n",
    "\n",
    "# To load the model:\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "model = load_model('cats_vs_dogs_model.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deepLearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
