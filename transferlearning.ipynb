{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transfer Learning Demo Using the VGG16 Model As A Base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\Anthony\\Documents\\CS Homework\\AI 260 Deep Learning S2024\\Transfer-Learning-6\\deepLearning\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\Anthony\\Documents\\CS Homework\\AI 260 Deep Learning S2024\\Transfer-Learning-6\\deepLearning\\Lib\\site-packages\\keras\\src\\backend.py:1398: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\Anthony\\Documents\\CS Homework\\AI 260 Deep Learning S2024\\Transfer-Learning-6\\deepLearning\\Lib\\site-packages\\keras\\src\\layers\\pooling\\max_pooling2d.py:161: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load the Pre-trained VGG16 Model\n",
    "# import VGG16 from keras.applications and load the model with ImageNet weights, excluding the top (fully connected) layers.\n",
    "\n",
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.image import resize\n",
    "from cv2 import imread, imshow\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Load the VGG16 model, pre-trained weights, exclude top layers\n",
    "base_model = VGG16(weights='imagenet', include_top=False, input_shape=(224, 224, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add Custom Layers\n",
    "# Freeze the layers of the base model to prevent them from being updated during training, then add your custom layers on top for the classification task.\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "# Freeze the base model\n",
    "base_model.trainable = False\n",
    "\n",
    "# Create a custom head for our network\n",
    "model = models.Sequential([\n",
    "  base_model,\n",
    "  layers.GlobalAveragePooling2D(),\n",
    "  layers.Dense(1024, activation='relu'),\n",
    "  layers.Dropout(0.2),\n",
    "  layers.Dense(2, activation='softmax')  # Assuming two classes: Cat and Dog\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\Anthony\\Documents\\CS Homework\\AI 260 Deep Learning S2024\\Transfer-Learning-6\\deepLearning\\Lib\\site-packages\\keras\\src\\optimizers\\__init__.py:309: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Compile and Train the Model\n",
    "# Compile the model, specifying the loss function, optimizer, and metrics. Then, train the model with your dataset.\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train/Test Split\n",
    "In order to adapt the VGG16 to classify cats and dogs, we used a Kaggle dataset that can be found at https://www.kaggle.com/c/dogs-vs-cats. The train.zip was downloaded and extracted and is referenced here as the \"dataset\" folder. This set has a rigid naming scheme and 25000 photos of dogs and cats, with 12500 of each. Each filename has a structure of (animal).(number).jpg, so they can be parsed to create a train/test split. We made an 80% train, 20% test split by counting all the cat and dog images with numbers below 10000 as part of the training set, and all remaining 2500 images of each as part of the test set. Labels were assigned by determining if the filenames started with cat or dog. Before being added to a set, each image was resized to 224 by 224 to match the input of the VGG16 model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images, test_images, train_labels, test_labels = [], [], [], []\n",
    "DIR = 'dataset/'\n",
    "\n",
    "def split_train_test():\n",
    "    for root, dir, files in os.walk('./dataset'):\n",
    "        for file in files:\n",
    "            img = imread(DIR + file)\n",
    "            processed = resize(\n",
    "                np.expand_dims(img, axis=0), [224, 224]\n",
    "            )\n",
    "            cat_or_dog = 1 if file.startswith('cat') else 0\n",
    "            # There are 12500 cats and dogs (indexed 0 to 12499) in the data set.\n",
    "            # To make an 80%:20% train-test split, 10000 cats and 10000 dogs need to go into the train set.\n",
    "            # All other cat and dog images go into the test set.\n",
    "            if int(file[file.index('.')+1:file.rindex('.')]) < 10000:\n",
    "                train_images.append(processed)\n",
    "                train_labels.append(cat_or_dog)\n",
    "            else:\n",
    "                test_images.append(processed)\n",
    "                test_labels.append(cat_or_dog)\n",
    "\n",
    "split_train_test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, a sample image and label are printed to make sure the data types in the sets are good. Each are converted to numpy arrays and reshaped as needed to match the VGG16 input shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "tf.Tensor(\n",
      "[[[[ 40.125     44.125     39.125   ]\n",
      "   [ 41.58817   44.061382  39.570312]\n",
      "   [ 43.848213  44.848213  40.848213]\n",
      "   ...\n",
      "   [180.87604  206.7868   210.63501 ]\n",
      "   [169.12376  201.10594  204.10594 ]\n",
      "   [163.03015  197.69086  200.69086 ]]\n",
      "\n",
      "  [[ 41.        45.        40.      ]\n",
      "   [ 42.017857  44.49107   40.      ]\n",
      "   [ 43.530132  44.530132  40.530132]\n",
      "   ...\n",
      "   [174.55463  200.35153  204.25667 ]\n",
      "   [165.6651   196.71312  200.27228 ]\n",
      "   [160.67636  194.21207  197.96207 ]]\n",
      "\n",
      "  [[ 40.268974  44.268974  39.268974]\n",
      "   [ 40.767857  43.24107   38.75    ]\n",
      "   [ 41.219868  42.219868  38.219868]\n",
      "   ...\n",
      "   [168.77898  192.79578  197.3259  ]\n",
      "   [164.99554  194.48663  198.97772 ]\n",
      "   [165.66518  197.32588  202.32588 ]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[ 28.785715  27.785715  29.785715]\n",
      "   [ 25.725447  24.725447  26.725447]\n",
      "   [ 21.830357  20.830357  22.830357]\n",
      "   ...\n",
      "   [ 25.214233  31.214233  44.214233]\n",
      "   [ 21.107178  27.107178  39.125   ]\n",
      "   [ 32.815792  38.815792  49.815792]]\n",
      "\n",
      "  [[ 30.660715  29.660715  31.660715]\n",
      "   [ 26.964287  25.964287  27.964287]\n",
      "   [ 21.925224  20.925224  22.925224]\n",
      "   ...\n",
      "   [ 20.878334  28.128334  41.128334]\n",
      "   [ 22.01683   29.573761  41.284653]\n",
      "   [ 35.315792  43.190792  53.565792]]\n",
      "\n",
      "  [[ 30.512278  29.512278  31.512278]\n",
      "   [ 26.089287  25.089287  27.089287]\n",
      "   [ 20.732143  19.732143  21.732143]\n",
      "   ...\n",
      "   [ 35.02449   43.02449   56.02449 ]\n",
      "   [ 26.112625  34.603714  46.130447]\n",
      "   [ 22.938618  31.938618  41.938618]]]], shape=(1, 224, 224, 3), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# Take a sample of one of the photos and labels to make sure the format is correct\n",
    "print(train_labels[1])\n",
    "print(train_images[1])\n",
    "\n",
    "# Convert labels to a numpy array, then convert image tensors of shape (x, 1, 224, 224, 3) to numpy arrays of shape (x, 224, 224, 3)\n",
    "train_labels = np.asarray(train_labels)\n",
    "train_images = np.array(train_images,dtype=np.float32).reshape((20000,224,224,3))\n",
    "test_labels = np.asarray(test_labels)\n",
    "test_images = np.array(test_images,dtype=np.float32).reshape((5000,224,224,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training and evaluating the augmented model\n",
    "Finally, we train the model using keras' ```model.fit``` function with the training images and labels, and using the test sets as validation data. The model training allocated over 11 GiB of memory to run and took just over 10 minutes to complete one epoch. After one epoch, the training accuracy came out to be 96.46%, with test accuracy coming out to 97.92%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\Anthony\\Documents\\CS Homework\\AI 260 Deep Learning S2024\\Transfer-Learning-6\\deepLearning\\Lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\Anthony\\Documents\\CS Homework\\AI 260 Deep Learning S2024\\Transfer-Learning-6\\deepLearning\\Lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "625/625 [==============================] - 785s 1s/step - loss: 0.2229 - accuracy: 0.9646 - val_loss: 0.0633 - val_accuracy: 0.9792\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x1aab7b4a710>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Assuming `train_images`, `train_labels` is the training dataset\n",
    "# Evaluate the model assuming `test_images`, `test_labels` is the testing dataset\n",
    "model.fit(train_images, train_labels, epochs=1, validation_data=(test_images,test_labels))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once trained, we could now save the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Anthony\\Documents\\CS Homework\\AI 260 Deep Learning S2024\\Transfer-Learning-6\\deepLearning\\Lib\\site-packages\\keras\\src\\engine\\training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    }
   ],
   "source": [
    "# Save and Load Your Model\n",
    "# Finally, save your trained model for later use and load it whenever you need to classify new images.\n",
    "model.save('cats_vs_dogs_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the model and making Analyses\n",
    "#### We will now load and use the model to make a prediction on an image passed to it. This is done via the predict() function built into keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To load the model:\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "model = load_model('cats_vs_dogs_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In the filepath variable below, the current configuration uses a string to represent a file being stored in the test folder. This string may be replaced with any filepath for the purpose of prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open image to predict\n",
    "filepath = \"test/88.jpg\"\n",
    "img = imread(filepath)\n",
    "processed_img = resize(np.expand_dims(img, axis=0), [224, 224])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### An important note when defining classes for prediciton is that the order matters. In this particular instance, due to the way that dogs and cats were specified during training. 1 representing cats and 0 representing dogs, thus making dogs come first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 106ms/step\n",
      "The input image is a dog\n"
     ]
    }
   ],
   "source": [
    "labels = ['dog','cat']\n",
    "prediction = labels[model.predict(processed_img).argmax()]\n",
    "print(\"The input image is a\", prediction)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deepLearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
